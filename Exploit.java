import java.util.Random;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.ArrayList;
import java.util.List;

import java.io.*;

import org.encog.util.simple.*;
import org.encog.neural.networks.*;
import org.encog.Encog;
import org.encog.ml.data.*;
import org.encog.ml.data.basic.BasicMLDataSet;
import org.encog.ml.data.basic.BasicMLData;
import org.encog.neural.networks.training.propagation.back.Backpropagation;
import org.encog.persist.EncogDirectoryPersistence;



public class Exploit
{
    public static Random r;
    private static BasicNetwork qtable;

    public static void main(String[] args)
    {
		qtable = (BasicNetwork)EncogDirectoryPersistence.loadObject(new File("saved_network.eg"));

        r = new Random();

        ArrayList<Integer> counts = new ArrayList<Integer>();

        //run one game
        for(int i=0; i < 1e5; i++)
        {
            int count = runGame();

            counts.add(count);

            if (i % 1000 == 0)
            {
                System.out.println(i + " iteration");
                System.out.println(count);
            }
        }

        int sum = 0;
        for (int i: counts)
        {
            sum += i;
        }

        System.out.println("average game length: "+sum / (float) counts.size());

        Encog.getInstance().shutdown();
    }

    private static int runGame()
    {

        Board board = new Board(r);
        board.fillRandom();

        int count = 0;

        while (!board.checkGameOver())
        {
            List<Board.MOVE> valid_moves = board.getValidMoves();

            Board.MOVE move;
            move = getBestMove(board, valid_moves);

            board.move(move);

            count++;
        }

        return count;
    }

    private static void printArray(double[] items)
    {
        for(double item: items)
        {
            System.out.print(item + ", ");
        }
        System.out.println();
    }


    private static Board.MOVE getBestMove(Board board, List<Board.MOVE> valid_moves)
    {
        double max = 0;

        Board.MOVE best_move = valid_moves.get(r.nextInt(valid_moves.size()));
        double[] state = board.neuralEncode();

        for (Board.MOVE move: valid_moves)
        {
            state[16] = move.ordinal() / 4.0;

            double[] result = qtable.compute(new BasicMLData(state)).getData();


            double expected = state[0] + state[1] * state[1] + 5;
            System.out.println(result[0] + ": " + expected);

            if(result[0] > max)
            {
                max = result[0];
                best_move = move;
            }
        }

        return best_move;
    }
}
